{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "São algorítimos versáteis que podem fazer classificação e regressão e até tarefas multioutputs. É o modelo base e fundamental das florestas, que são muito poderosas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando e visualizando uma árvore de decisão\n",
    "\n",
    "Para entender árvore de decisão vamos construir uma e olhar a forma como ela faz predições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(array([1.4, 0.2]), 0),\n (array([1.4, 0.2]), 0),\n (array([1.3, 0.2]), 0),\n (array([1.5, 0.2]), 0),\n (array([1.4, 0.2]), 0),\n (array([1.7, 0.4]), 0),\n (array([1.4, 0.3]), 0),\n (array([1.5, 0.2]), 0),\n (array([1.4, 0.2]), 0),\n (array([1.5, 0.1]), 0),\n (array([1.5, 0.2]), 0),\n (array([1.6, 0.2]), 0),\n (array([1.4, 0.1]), 0),\n (array([1.1, 0.1]), 0),\n (array([1.2, 0.2]), 0),\n (array([1.5, 0.4]), 0),\n (array([1.3, 0.4]), 0),\n (array([1.4, 0.3]), 0),\n (array([1.7, 0.3]), 0),\n (array([1.5, 0.3]), 0),\n (array([1.7, 0.2]), 0),\n (array([1.5, 0.4]), 0),\n (array([1. , 0.2]), 0),\n (array([1.7, 0.5]), 0),\n (array([1.9, 0.2]), 0),\n (array([1.6, 0.2]), 0),\n (array([1.6, 0.4]), 0),\n (array([1.5, 0.2]), 0),\n (array([1.4, 0.2]), 0),\n (array([1.6, 0.2]), 0),\n (array([1.6, 0.2]), 0),\n (array([1.5, 0.4]), 0),\n (array([1.5, 0.1]), 0),\n (array([1.4, 0.2]), 0),\n (array([1.5, 0.2]), 0),\n (array([1.2, 0.2]), 0),\n (array([1.3, 0.2]), 0),\n (array([1.4, 0.1]), 0),\n (array([1.3, 0.2]), 0),\n (array([1.5, 0.2]), 0),\n (array([1.3, 0.3]), 0),\n (array([1.3, 0.3]), 0),\n (array([1.3, 0.2]), 0),\n (array([1.6, 0.6]), 0),\n (array([1.9, 0.4]), 0),\n (array([1.4, 0.3]), 0),\n (array([1.6, 0.2]), 0),\n (array([1.4, 0.2]), 0),\n (array([1.5, 0.2]), 0),\n (array([1.4, 0.2]), 0),\n (array([4.7, 1.4]), 1),\n (array([4.5, 1.5]), 1),\n (array([4.9, 1.5]), 1),\n (array([4. , 1.3]), 1),\n (array([4.6, 1.5]), 1),\n (array([4.5, 1.3]), 1),\n (array([4.7, 1.6]), 1),\n (array([3.3, 1. ]), 1),\n (array([4.6, 1.3]), 1),\n (array([3.9, 1.4]), 1),\n (array([3.5, 1. ]), 1),\n (array([4.2, 1.5]), 1),\n (array([4., 1.]), 1),\n (array([4.7, 1.4]), 1),\n (array([3.6, 1.3]), 1),\n (array([4.4, 1.4]), 1),\n (array([4.5, 1.5]), 1),\n (array([4.1, 1. ]), 1),\n (array([4.5, 1.5]), 1),\n (array([3.9, 1.1]), 1),\n (array([4.8, 1.8]), 1),\n (array([4. , 1.3]), 1),\n (array([4.9, 1.5]), 1),\n (array([4.7, 1.2]), 1),\n (array([4.3, 1.3]), 1),\n (array([4.4, 1.4]), 1),\n (array([4.8, 1.4]), 1),\n (array([5. , 1.7]), 1),\n (array([4.5, 1.5]), 1),\n (array([3.5, 1. ]), 1),\n (array([3.8, 1.1]), 1),\n (array([3.7, 1. ]), 1),\n (array([3.9, 1.2]), 1),\n (array([5.1, 1.6]), 1),\n (array([4.5, 1.5]), 1),\n (array([4.5, 1.6]), 1),\n (array([4.7, 1.5]), 1),\n (array([4.4, 1.3]), 1),\n (array([4.1, 1.3]), 1),\n (array([4. , 1.3]), 1),\n (array([4.4, 1.2]), 1),\n (array([4.6, 1.4]), 1),\n (array([4. , 1.2]), 1),\n (array([3.3, 1. ]), 1),\n (array([4.2, 1.3]), 1),\n (array([4.2, 1.2]), 1),\n (array([4.2, 1.3]), 1),\n (array([4.3, 1.3]), 1),\n (array([3. , 1.1]), 1),\n (array([4.1, 1.3]), 1),\n (array([6. , 2.5]), 2),\n (array([5.1, 1.9]), 2),\n (array([5.9, 2.1]), 2),\n (array([5.6, 1.8]), 2),\n (array([5.8, 2.2]), 2),\n (array([6.6, 2.1]), 2),\n (array([4.5, 1.7]), 2),\n (array([6.3, 1.8]), 2),\n (array([5.8, 1.8]), 2),\n (array([6.1, 2.5]), 2),\n (array([5.1, 2. ]), 2),\n (array([5.3, 1.9]), 2),\n (array([5.5, 2.1]), 2),\n (array([5., 2.]), 2),\n (array([5.1, 2.4]), 2),\n (array([5.3, 2.3]), 2),\n (array([5.5, 1.8]), 2),\n (array([6.7, 2.2]), 2),\n (array([6.9, 2.3]), 2),\n (array([5. , 1.5]), 2),\n (array([5.7, 2.3]), 2),\n (array([4.9, 2. ]), 2),\n (array([6.7, 2. ]), 2),\n (array([4.9, 1.8]), 2),\n (array([5.7, 2.1]), 2),\n (array([6. , 1.8]), 2),\n (array([4.8, 1.8]), 2),\n (array([4.9, 1.8]), 2),\n (array([5.6, 2.1]), 2),\n (array([5.8, 1.6]), 2),\n (array([6.1, 1.9]), 2),\n (array([6.4, 2. ]), 2),\n (array([5.6, 2.2]), 2),\n (array([5.1, 1.5]), 2),\n (array([5.6, 1.4]), 2),\n (array([6.1, 2.3]), 2),\n (array([5.6, 2.4]), 2),\n (array([5.5, 1.8]), 2),\n (array([4.8, 1.8]), 2),\n (array([5.4, 2.1]), 2),\n (array([5.6, 2.4]), 2),\n (array([5.1, 2.3]), 2),\n (array([5.1, 1.9]), 2),\n (array([5.9, 2.3]), 2),\n (array([5.7, 2.5]), 2),\n (array([5.2, 2.3]), 2),\n (array([5. , 1.9]), 2),\n (array([5.2, 2. ]), 2),\n (array([5.4, 2.3]), 2),\n (array([5.1, 1.8]), 2)]"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# visualize the data and target\n",
    "list(zip(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "treinando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth = 3)\n",
    "tree_clf.fit(X, y)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exportando desenho da árvore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file = 'iris_tree.dot',\n",
    "    feature_names = iris.feature_names[2:],\n",
    "    class_names = iris.target_names,\n",
    "    rounded = True,\n",
    "    filled = True\n",
    ")\n",
    "\n",
    "# comando bash:\n",
    "# dot -Tpng iris_tree.dot -o iris_tree.png\n",
    "# olhar iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previsões\n",
    "\n",
    "As previsões em árvores são feitas conforme o esquemático mostrado acima. Cada nó faz uma pergunta e bifurca para outros nós a depender da resposta.\n",
    "\n",
    "Uma vantagem das árvores é que elas precisam de pouca preparação dos dados e não se importam com feature scaling ou feature centering.\n",
    "\n",
    "O atributo 'samples' dos nós contam quantas instâncias chegaram nesse nó.\n",
    "\n",
    "O atributo 'gini' mede a impureza do nó. Se gini = 0, todas as instâncias pertencem a mesma classe. Veja a fórmula do gini:\n",
    "\n",
    "$$\n",
    "G_i = 1 - \\sum_{k=1}^{n}p_{i,k}^2\n",
    "$$\n",
    "\n",
    "* $p_{i,k}$ é a porcentagem de elementos da classe k nos samples do i-ésimo nó\n",
    "\n",
    "suponha que vc tem 3 classes possíveis e em um nó temos 54 samples. Sendo (classe1: 0, classe2: 49, classe3: 5). Assim, a impureza é de\n",
    "\n",
    " $1 - (\\frac{0}{54})^2 - (\\frac{49}{54})^2- (\\frac{5}{54})^2 \\approx 0.168$\n",
    "\n",
    "O sklearn usa o algoritmo CART que produz apenas árvores binárias. Outros algoritmos como ID3 podem produzir árvores não binárias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se a produndidade fosse até 3, as linhas verticais pontilhadas seriam feitas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/treeboundary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Árvores são modelos transparentes, 'white box'. Diferentemente das florestas e das redes neurais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimando as probabilidades das classes\n",
    "\n",
    "Para uma instância, a árvore primeiro tenta encaixá-la em uma folha e, em seguida, cospe sua probabilidade baseada na pureza dessa folha nos dados de treino.\n",
    "\n",
    "usando o exemplo anterior: uma flor com 5cm x 1.5cm de dimensão é colocada no nó da esquerda de profundidade 2. Assim, usando o exemplo (0, 49, 5), a árvore vai dizer que essa instância tem 49/54 % de chance de ser Versicolor (classe2) e 5/54 % de chance de ser virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.        , 0.33333333, 0.66666667]])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O algoritmo CART\n",
    "\n",
    "O algoritmo CART (classification And Regression Tree) é usado para treinar as árvores de decisão. A ideia é simples: o algoritmo separa o dataset de treino em 2 subconjuntos usando uma única feature $k$ e um threshold $t_k$. Como ele escolhe $k$ e $t_k$? Ele busca pelo par $(k, t_k)$ que produz os subconjuntos mais puros (usando o tamanho como peso). A função de custo que o algoritmo tenta minimizar é dada por:\n",
    "\n",
    "$$\n",
    "J(k, t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}\n",
    "$$\n",
    "\n",
    "* $G_{left/right}$ mede a impureza do subconjunto esquerdo/direito\n",
    "* $m_{left/right}$ é o número de instâncias no subconjunto da esquerda/direita\n",
    "\n",
    "Uma vez que se consegue fazer isso, ela separa os subconjuntos de novo usando a mesma lógica. Isso para quando chega a profundidade máxima (hyperparameter) ou se não consegue encontrar uma divisão que reduza impureza.\n",
    "\n",
    "Percebam que CART é um algoritmo greedy/guloso. Ele procura por uma divisão ótima a cada profundidade, sem saber se isso vai resultar em impurezas pequenas em maiores profundidades. Não há garantia de ser uma solução ótima.\n",
    "\n",
    "Há como encontrar a árvore perfeita! Pena que a ordem complexidade é $O(exp(m))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexidade\n",
    "\n",
    "Predição: $O(log_2(m))$, muito rápido. Independe do número de features(n)\n",
    "\n",
    "Treino: $O(n\\times mlog(m))$. Ele compara todas as features em todos os samples em cada nó (a menos que max_features seja determinado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini impurity or Entropy?\n",
    "\n",
    "O default é o Gini, mas vc pode selecionar a entropia. (criterion = 'entropy'). Entropia para o i-ésimo nó é dada por:\n",
    "\n",
    "$$\n",
    "H_i = - \\sum_{k=1}^n p_{i,k} log(p_{i, k})\n",
    "$$\n",
    "\n",
    "Qual usar? tanto faz. As diferenças são ínfimas. Mas gini tende a ser mais rápido e entropia a deixar a arvore mais balanceada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparâmetros de regularização\n",
    "\n",
    "Árvore assume pouca coisa sobre os dados e, por isso, se deixá-la sem constraints, ela vai overfittar.\n",
    "\n",
    "max_depth -> profundidade máxima da árvore.\n",
    "\n",
    "min_samples_split -> número de instâncias um nó deve ter antes de se quebrar\n",
    "\n",
    "min_samples_leaf -> número mínimo de instâncias que uma folha deve ter.\n",
    "\n",
    "min_weight_fraction_leaf -> mesmo do de cima mas expresso como uma fração do total de instancias\n",
    "\n",
    "max_leaf_nodes -> máximo de folhas.\n",
    "\n",
    "max_features -> máximo de features que são observadas para splittar em cada nó\n",
    "\n",
    "regularizar: aumentar min's ou diminuir max's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns algoritmos podem gerar uma árvore sem regularização e, em seguida, podá-la, retirando nós que não trazem ganhos reais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exemplo:\n",
    "\n",
    "![](./imgs/treeover.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão\n",
    "\n",
    "Árvores podem fazer regressão e, para isso, elas determinam um valor para cada região no nosso espaço de features.\n",
    "\n",
    "exemplo:\n",
    "\n",
    "![](./imgs/treereg.png)\n",
    "\n",
    "Continuamos usando o CART mas ao invés de minimizar impureza minimizamos o MSE. Função de custo:\n",
    "\n",
    "$$\n",
    "J(k, t_k) = \\frac{m_{left}}{m}MSE_{left} + \\frac{m_{right}}{m}MSE_{right}\n",
    "$$\n",
    "\n",
    "exemplo de regressão:\n",
    "\n",
    "![](./imgs/treereg2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instabilidade\n",
    "\n",
    "Percebemos que árvores de decisão amam decision boundaries ortogonais. Isso as faz muito sensíveis a rotação dos dados.\n",
    "\n",
    "exemplo:\n",
    "\n",
    "![](./imgs/treerot.png)\n",
    "\n",
    "Uma forma de melhorar nesse quesito é usar PCA, que frequentemente resulta em uma orientação melhor dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "1. Qual a profundidade de uma árvore de decisão treinada sem restrições sobre um dataset de 1M de instâncias?\n",
    "\n",
    "r: $log_2(10^6)$\n",
    "\n",
    "2. O gini de um nó filho é maior ou menor que o do pai?\n",
    "\n",
    "r: normalmente menor, mas a função de custo visa diminuir a soma ponderada. Portanto, um filho pode ter gini mais alto que o do pai se o outro tiver gini baixo.\n",
    "\n",
    "3. Diminuir max_depth ajuda contra overfitting?\n",
    "\n",
    "r: sim, é uma forma de regularizar!\n",
    "\n",
    "4. Se uma árvore está underfittando, faz sentido padronizar as features?\n",
    "\n",
    "r: Não. Em geral não surte muito efeito.\n",
    "\n",
    "5. qual a complexidade do treino?\n",
    "\n",
    "r: $O(n \\times mlog(m))$\n",
    "\n",
    "6. Com 100k linhas, presort = True irá acelerar o treino?\n",
    "\n",
    "r: Não. Só acelera para datasets pequenos (<10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitanaconda3virtualenv2d4ee57395ed49639435b8b2b96a9cbb",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}