{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 8: redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um problema recorrente no ML é a quantidade obscena de features que precisamos lidar em alguns datasets.\n",
    "\n",
    "Isso não só torna o treino lento como pode dificultar a busca de boas soluções.\n",
    "\n",
    "Além disso, redução de dimensionalidade é extremamente útil para visualização!\n",
    "\n",
    "Apelidamos o problema de 'maldição da dimensionalidade' (*curse of dimensionality*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exemplo: \n",
    "\n",
    "para identificar um número no mnist podemos: \n",
    "\n",
    "* descartar as bordas brancas\n",
    "* diminuir a resolução da imagem\n",
    "\n",
    "e, ainda assim, ter informação suficiente para determinar de qual dígito se trata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "\n",
    "Algumas coisas se comportam de forma diferente em espaços de alta dimensionalidade.\n",
    "\n",
    "Escolha um ponto aleatório dentro de um quadrado 1x1. ele terá 0.4% de chance de estar a menos de 0.001 de uma borda.\n",
    "\n",
    "No entanto, em um hipercubo com 10k dimensões, a probabilidade disso ocorrer sobe para 99.999999%\n",
    "\n",
    "No código abaixo a gente compara entre 1k dimensões vs 2 dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "chance para 10k dimensões:  0.86473\nchance para 2 dimensões:  0.00386\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import random as rnd\n",
    "\n",
    "d1k = rnd.uniform(size = (1000, 100000))\n",
    "d2 = rnd.uniform(size = (2, 100000))\n",
    "\n",
    "print('chance para 10k dimensões: ', sum(sum(np.logical_or(d1k<0.001, d1k> 1- 0.001)) != 0)/100000)\n",
    "\n",
    "print('chance para 2 dimensões: ', sum(sum(np.logical_or(d2<0.001, d2> 1- 0.001)) != 0)/100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em um cubo unitário 2d (quadrado), a distância média entre 2 pontos aleatórios é de 0.52 \n",
    "\n",
    "Para o cubo 3d: 0.66\n",
    "\n",
    "Para o cubo 1000000d: 408.25\n",
    "\n",
    "\n",
    "O fato é: datasets de alta dimensionalidade correm o risco de serem esparsos!\n",
    "\n",
    "As instâncias de treino estarão distantes entre si e, provavelmente, novas instâncias surgirão longe de todos os dados de treino.\n",
    "\n",
    "Isso tem um impacto forte sobre a confiabilidade das previsões feitas. Como podemos prever instâncias distantes das instâncias que conhecíamos?\n",
    "\n",
    "Em resumo: maior dimensionalidade $\\implies$ mais chance de overfittar\n",
    "\n",
    "Na teoria podemos aumentar o número de instâncias para aumentar a densidade de dados. Na prática o número de dados necessários para manter a densidade acompanhando a dimensão cresce exponencialmente com a dimensão :/.\n",
    "\n",
    "Com 100 features, você precisa de mais dados de treino do que o número de átomos no universo observável para conseguir instâncias espaçadas por 0.1 entre elas na média. (assumindo dispersão uniforme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principais abordagens para redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projeção\n",
    "\n",
    "As vezes é possível acomodar a distribuição dos dados em espaços menores que o espaço original. exemplo:\n",
    "\n",
    "![](./imgs/subspace.png)\n",
    "\n",
    "![](./imgs/subspace2.png)\n",
    "\n",
    "No entanto, nem sempre é possível projetar de forma legal...\n",
    "\n",
    "![](./imgs/proj.png)\n",
    "\n",
    "Queremos uma forma de 'desenrolar' esse rolo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learning\n",
    "\n",
    "Esse rolo acima é um exemplo de um manifold 2d. Um manifold é um formato 2d que pode ser dobrado e espremido em um espaço dimensional maior. Como um pano de cozinha.\n",
    "\n",
    "Em modo mais geral é um objeto d-dimensional forçadamente inserido em um espaço n-dimensional com $n \\gt d$.\n",
    "\n",
    "Os 'constraints implícitos' de um conjunto de dados 'obriga' os dados de alta dimensão a serem acomodáveis em uma dimensão menor que a que eles pertencem. \n",
    "\n",
    "No MNIST, por exemplo, os números são escritos com linhas contínuas, isso implica que os pixels não possuem tantos graus de liberdade quanto poderiam. Logo eles devem se acomodar em um espaço dimensional menor.\n",
    "\n",
    "Usar manifold vem da crença que o dataset vai ser mais facilmente explicável em dimensões menores, o que nem sempre é verdade. Vejamos na imagem:\n",
    "\n",
    "![](./imgs/manifold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Principals Component Analysis (PCA) é de longe o algoritmo de redução de dimensionalidade mais famoso.\n",
    "\n",
    "Primeiro ele identifica o hiperplanos que melhor acomoda o dataset e, em seguida, projeta os dados nele.\n",
    "\n",
    "![](./imgs/pca1.png)\n",
    "\n",
    "Parece justificável escolher o eixo que preserva a maior variância. Outra forma de justificar a escolha é que esse eixo minimiza a distância quadrática dos dados e suas projeções no eixo.\n",
    "\n",
    "Essa é a ideia por trás do PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal components\n",
    "\n",
    "O PCA acha n eixos (vetores) ortogonais entre si para projetar os dados. o i-ésimo vetor é chamado de i-ésimo componente principal (PC).\n",
    "\n",
    "A técnica usada para achar os PC's é chamada de SVD (singular value decomposition). É basicamente achar os auto-vetores e auto-valores da matriz.\n",
    "\n",
    "**Importante:** PCA assume que os dados estão centralizados. Portanto, centralize seus dados antes de usá-lo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como reduzir a dimensão?\n",
    "\n",
    "Descarta os últimos componentes principais. Isso fará com que você preserve o máximo de variância possível ao eliminar features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_s_curve\n",
    "\n",
    "X, y = make_s_curve(5000)\n",
    "pca = PCA(n_components = 2)\n",
    "X2d = pca.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax3d = Axes3D(fig)\n",
    "ax3d.scatter(X[:,0], X[:,1], X[:,2], c = y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X2d[:,0], X2d[:,1], c = y )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio\n",
    "\n",
    "Indica a proporção do dataset que se acomoda ao longo do eixo de cada componente principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "evr por componente:  [0.69909465 0.17868298] \n\nVariância total preservada:  0.8777776350977695\n"
    }
   ],
   "source": [
    "evr = pca.explained_variance_ratio_\n",
    "\n",
    "print('evr por componente: ',evr,'\\n\\nVariância total preservada: ', sum(evr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolhendo o número correto de dimensões\n",
    "\n",
    "Regra de bolso: número que cubra 95% da variância. (usar explained_variance_ratio)\n",
    "\n",
    "pca = PCA(n_components = 0.95)\n",
    "\n",
    "Outra opção é plotar o explained_variance_ratio vs número de componentes e encontrar no plot um número interessante de componentes.\n",
    "\n",
    "![](./imgs/pca2.png)\n",
    "\n",
    "perceba que isso é mais um trade-off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA para compressão\n",
    "\n",
    "Reduzir a dimensão é, também, comprimir os dados. Usando PCA para o MNIST com 95% de variância preservada reduz de 784 features para 150!\n",
    "\n",
    "jogamos fora mais de 80% do dataset, acelerando pakas o processo de treino.\n",
    "\n",
    "tem como 'descomprimir' para as 784 dimensões e visualizar o que foi perdido.\n",
    "\n",
    "Resultado:\n",
    "\n",
    "![](./imgs/pca3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais PCA\n",
    "\n",
    "#### Incremental\n",
    "\n",
    "É possível treinar em mini-batches. Bom para rodar online e para datasets grandes.\n",
    "\n",
    "#### Randomized\n",
    "\n",
    "Estocástico. Encontra um valor aproximado para os primeiros d componentes principais.\n",
    "\n",
    "#### Kernel\n",
    "\n",
    "Lembram do kernel trick do SVM?\n",
    "\n",
    "O mesmo truque serve pro PCA, possibilitando fazer projeções não lineares complexas para a redução de dimensionalidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.datasets import make_friedman3\n",
    "\n",
    "X, y = make_swiss_roll(5000, random_state=0)\n",
    "pca = KernelPCA(n_components = 2, kernel = 'sigmoid', gamma = 1E-3)\n",
    "X2d = pca.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X2d[:,0], X2d[:,1], c = y )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecionando um kernel e tunando hiperparâmetros\n",
    "\n",
    "grid_search.\n",
    "\n",
    "KernelPCA é unsupervised mas vc pode usar a previsão do modelo seguinte da pipeline para verificar quais são os melhores parâmetros para o KernelPCA.\n",
    "\n",
    "Uma abordagem completamente unsupervised é selecionar o kernel e hiperparâmetros que acusam o menor erro de reconstrução - reconstruir não é tão trivial nesse caso. \n",
    "\n",
    "Página 219 para mais informações sobre isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE - Locally Linear Embedding\n",
    "\n",
    "Técnica de manifold learning que não se baseia em projeção. Esse modelo mede como cada instância de treino se relaciona linearmente com seu vizinho mais próximo.\n",
    "\n",
    "Então procura por uma representação de menor dimensão onde essas relações locais são melhores preservadas.\n",
    "\n",
    "É especialmente boa para desenrolar datasets enrolados, especialmente quando se tem pouco ruído."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "X, y = make_swiss_roll(5000, random_state=0)\n",
    "\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X2d = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax3d = Axes3D(fig)\n",
    "ax3d.scatter(X[:,0], X[:,1], X[:,2], c = y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X2d[:,0], X2d[:,1], c = y )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algorítmo identifica os K vizinhos mais próximos de $\\pmb{x}^{(i)}$ e tenta reconstruir $\\pmb{x}^{(i)}$ como uma função linear desses vizinhos.\n",
    "\n",
    "Encontra pesos que multiplicam os k vizinhos para tentar igualar a $\\pmb{x}^{(i)}$.\n",
    "\n",
    "Criamos um vetor para cada instância. Ou seja, criamos uma matriz. Essa matriz codifica as relações locais lineares entre as instâncias de treino.\n",
    "\n",
    "$$\n",
    "\\hat{\\pmb{W}} =  \\underset{W}{\\mathrm{argmin}}\\sum_{i=1}^{m}||\\pmb{x}^{(i)} - \\sum_{j = 1}^{m} w_{i,j}\\pmb x^{(j)}||^2\n",
    "$$\n",
    "\n",
    "Mas $w_{i,j} = 0$ para quando $x^{(j)}$ não é um dos vizinhos de $x^{(i)}$.\n",
    "\n",
    "O próximo passo é mapear as instâncias em um espaço d-dimensional ($d \\lt n$) preservando essas relações lineares.\n",
    "\n",
    "Criamos uma instância $\\pmb{z}^{(i)}$ para cada $\\pmb{x}^{(i)}$ em um novo espaço d-dimensional (z é a imagem de x nesse novo espaço). Com o vetor de pesos fixo tentamos descrever $\\pmb{z}^{(i)}$ através de seus vizinhos.\n",
    "\n",
    "$$\n",
    "\\hat{\\pmb{Z}} =  \\underset{Z}{\\mathrm{argmin}}\\sum_{i=1}^{m}||\\pmb{Z}^{(i)} - \\sum_{j = 1}^{m} w_{i,j}\\pmb Z^{(j)}||^2\n",
    "$$\n",
    "\n",
    "Basicamente ele tenta achar um lugar legal para colocar a imagem de x nesse novo espaço."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outras técnicas\n",
    "\n",
    "* Multidimensional Scaling (MDS) reduces dimensionality while trying to preserve\n",
    "the distances between the instances\n",
    "\n",
    "* Isomap creates a graph by connecting each instance to its nearest neighbors, then\n",
    "reduces dimensionality while trying to preserve the geodesic distances between\n",
    "the instances.\n",
    "\n",
    "* t-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality\n",
    "while trying to keep similar instances close and dissimilar instances apart. It is\n",
    "mostly used for visualization, in particular to visualize clusters of instances in\n",
    "high-dimensional space\n",
    "\n",
    "* Linear Discriminant Analysis (LDA) is actually a classification algorithm, but dur‐\n",
    "ing training it learns the most discriminative axes between the classes, and these\n",
    "axes can then be used to define a hyperplane onto which to project the data. The\n",
    "benefit is that the projection will keep classes as far apart as possible, so LDA is a\n",
    "good technique to reduce dimensionality before running another classification\n",
    "algorithm such as an SVM classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitanaconda3virtualenv2d4ee57395ed49639435b8b2b96a9cbb",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}